{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d8474fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "\n",
    "import string\n",
    "import spacy\n",
    "import sklearn\n",
    "import tqdm\n",
    "import math\n",
    "\n",
    "from datasets import Dataset\n",
    "import joblib\n",
    "import tqdm\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "import os\n",
    "\n",
    "# from nltk.tokenize import TreebankWordTokenizer, RegexpTokenizer\n",
    "# from nltk.tokenize.casual import casual_tokenize\n",
    "# from nltk.util import ngrams\n",
    "\n",
    "punctuations = (string.punctuation + '‚Äú' + '‚Ä¶')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b408679c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spacy.lang.en.stop_words import STOP_WORDS as en_stop_words\n",
    "# from spacy.lang.tl.stop_words import STOP_WORDS as tl_stop_words\n",
    "# for w in (['ba','eh','kasi','lang','mo','naman','opo','po','si','talaga','yung']):\n",
    "#     tl_stop_words.add(w)\n",
    "\n",
    "# stop_words = en_stop_words.union(tl_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e9a4f5",
   "metadata": {},
   "source": [
    "# Baseline Preproccessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2a606b",
   "metadata": {},
   "source": [
    "I also tried versions with stop word removal, lemmatization, etc. No real benefits with BERT models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3c57ef12",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for fname in list(os.walk('data/hashtags'))[0][2]:\n",
    "    dfs.append(pd.read_csv(f'data/hashtags/{fname}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4daf82c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat(dfs).drop_duplicates(subset=['id']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "55e68463",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['tweet', 'hashtags']].copy()\n",
    "data['hashtags'] = data['hashtags'].apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8bb44673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to view the tags\n",
    "tags = data['hashtags'].explode().dropna().str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "68aad1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['hashtags_lower'] = data['hashtags'].apply(lambda l: [s.lower() for s in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "477f2d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually filter through the tags to flag positive and negative tags\n",
    "anti_tags = ['lenikiko2022', 'kulayrosasangbukas', 'leniforpresident2022', 'lenikikoalltheway', 'kaylenitayo']\n",
    "pro_tags = ['bbmsarauniteam', 'bbmismypresident2022', 'bbmsara2022', 'bringbackmarcos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6700bdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['anti'] = data['hashtags_lower'].map(lambda l: any([s in anti_tags for s in l]))\n",
    "data['pro'] = data['hashtags_lower'].map(lambda l: any([s in pro_tags for s in l]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0aceda86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark all of the tweets with only one tag affiliation, and re-mark pro and anti tweets as such\n",
    "data['labeled'] = data['anti'] ^ data['pro']\n",
    "data['anti'] = data['anti'] & data['labeled']\n",
    "data['pro'] = data['pro'] & data['labeled']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "50013dac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>hashtags_lower</th>\n",
       "      <th>anti</th>\n",
       "      <th>pro</th>\n",
       "      <th>labeled</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ang tatay at nanay ko nga hindi din pumila kas...</td>\n",
       "      <td>[bbmsarauniteam, bbmismypresident2022]</td>\n",
       "      <td>[bbmsarauniteam, bbmismypresident2022]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hindi pa tapos ang eleksyon may sumipsip na. #...</td>\n",
       "      <td>[bbmismypresident2022]</td>\n",
       "      <td>[bbmismypresident2022]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E pumila din naman ang presidente ko  #Halalan...</td>\n",
       "      <td>[halalan2022, bbmsarauniteam, bbmismypresident...</td>\n",
       "      <td>[halalan2022, bbmsarauniteam, bbmismypresident...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Titindi ang Labanan sa Pagitan nang ‚ù§Ô∏èüíö at üíõüå∫ ...</td>\n",
       "      <td>[votewisely2022, uniteam, bbmsarauniteam, bbmi...</td>\n",
       "      <td>[votewisely2022, uniteam, bbmsarauniteam, bbmi...</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#Halalan2022  #BBMIsMyPresident2022</td>\n",
       "      <td>[halalan2022, bbmismypresident2022]</td>\n",
       "      <td>[halalan2022, bbmismypresident2022]</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  \\\n",
       "0  Ang tatay at nanay ko nga hindi din pumila kas...   \n",
       "1  Hindi pa tapos ang eleksyon may sumipsip na. #...   \n",
       "2  E pumila din naman ang presidente ko  #Halalan...   \n",
       "3  Titindi ang Labanan sa Pagitan nang ‚ù§Ô∏èüíö at üíõüå∫ ...   \n",
       "4                #Halalan2022  #BBMIsMyPresident2022   \n",
       "\n",
       "                                            hashtags  \\\n",
       "0             [bbmsarauniteam, bbmismypresident2022]   \n",
       "1                             [bbmismypresident2022]   \n",
       "2  [halalan2022, bbmsarauniteam, bbmismypresident...   \n",
       "3  [votewisely2022, uniteam, bbmsarauniteam, bbmi...   \n",
       "4                [halalan2022, bbmismypresident2022]   \n",
       "\n",
       "                                      hashtags_lower   anti   pro  labeled  \n",
       "0             [bbmsarauniteam, bbmismypresident2022]  False  True     True  \n",
       "1                             [bbmismypresident2022]  False  True     True  \n",
       "2  [halalan2022, bbmsarauniteam, bbmismypresident...  False  True     True  \n",
       "3  [votewisely2022, uniteam, bbmsarauniteam, bbmi...  False  True     True  \n",
       "4                [halalan2022, bbmismypresident2022]  False  True     True  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_data = data[data['labeled']]\n",
    "labeled_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a8e6cf8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Conner\\AppData\\Local\\Temp\\ipykernel_3720\\2897153354.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  labeled_data['processed'] = labeled_data['tweet'].str.replace('\\s', ' ').str.replace(r'#\\w*', '').str.replace(r'https?://\\S+', \"\")\n",
      "C:\\Users\\Conner\\AppData\\Local\\Temp\\ipykernel_3720\\2897153354.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  labeled_data['processed'] = labeled_data['tweet'].str.replace('\\s', ' ').str.replace(r'#\\w*', '').str.replace(r'https?://\\S+', \"\")\n",
      "C:\\Users\\Conner\\AppData\\Local\\Temp\\ipykernel_3720\\2897153354.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  labeled_data['label'] = labeled_data['pro'].astype(int)\n"
     ]
    }
   ],
   "source": [
    "labeled_data['processed'] = labeled_data['tweet'].str.replace('\\s', ' ').str.replace(r'#\\w*', '').str.replace(r'https?://\\S+', \"\")\n",
    "labeled_data['label'] = labeled_data['pro'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c63b88d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_data = labeled_data.drop_duplicates(subset='processed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2f5161a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = Dataset.from_dict({\n",
    "    'text': labeled_data['processed'],\n",
    "    'label': labeled_data['label']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2eb1068d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['raw_dataset.pkl']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(raw_dataset, 'raw_dataset.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c048660d",
   "metadata": {},
   "source": [
    "# Language Detection and Dual Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a9d9d5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import six\n",
    "from google.cloud import translate_v2 as translate\n",
    "from google.api_core.exceptions import ServiceUnavailable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "753e4583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'path/to/your/credentials'\n",
    "os.path.isfile(os.environ['GOOGLE_APPLICATION_CREDENTIALS'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c3d04257",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 61025\n",
       "})"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "f930fdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text(target, text):\n",
    "    \"\"\"Translates text into the target language.\n",
    "\n",
    "    Target must be an ISO 639-1 language code.\n",
    "    See https://g.co/cloud/translate/v2/translate-reference#supported_languages\n",
    "    \"\"\"\n",
    "    \n",
    "    translate_client = translate.Client()\n",
    "\n",
    "    if isinstance(text, six.binary_type):\n",
    "        text = text.decode(\"utf-8\")\n",
    "\n",
    "    # Text can also be a sequence of strings, in which case this method\n",
    "    # will return a sequence of results for each text.\n",
    "    result = translate_client.translate(text, target_language=target)\n",
    "\n",
    "#     print(u\"Text: {}\".format(result[\"input\"]))\n",
    "#     print(u\"Translation: {}\".format(result[\"translatedText\"]))\n",
    "#     print(u\"Detected source language: {}\".format(result[\"detectedSourceLanguage\"]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1c4da8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_en_translation_and_lang(sample):\n",
    "    try:\n",
    "        text = sample['text']\n",
    "        res = translate_text('en', text)\n",
    "        sample['en_translation'] = res['translatedText']\n",
    "        sample['lang'] = res[\"detectedSourceLanguage\"]\n",
    "    except ServiceUnavailable:\n",
    "        sample['en_translation'] = None\n",
    "        sample['lang'] = None\n",
    "    except Exception as e:\n",
    "        sample['en_translation'] = e\n",
    "        sample['lang'] = e\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "a7cf6a7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.031009674072265625,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 61025,
       "unit": "ex",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65c58cc786a04de39a90e451b3e367ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61025 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "en_translated_dataset = raw_dataset.map(add_en_translation_and_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "aca2f27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_en_translation(sample):\n",
    "    if sample['lang'] is None:\n",
    "        sample = add_en_translation_and_lang(sample)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "823326d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.04162025451660156,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 61025,
       "unit": "ex",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34333f38ef804b36be1c7fc800b17896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61025 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "en_translated_dataset = en_translated_dataset.map(fix_en_translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "049c331c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_tl_translation(sample):\n",
    "    try:\n",
    "        if sample['lang'] =='fil':\n",
    "            sample['tl_translation'] = translate_text('tl', sample['en_translation'])['translatedText']\n",
    "        else:\n",
    "            sample['tl_translation'] = translate_text('tl', sample['text'])['translatedText']\n",
    "    except ServiceUnavailable:\n",
    "        sample['tl_translation'] = None\n",
    "    except Exception as e:\n",
    "        sample['tl_translation'] = 0\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "16d047b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.019999980926513672,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 61025,
       "unit": "ex",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a3415482a69440f9767cd65db70dd06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/61025 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dual_translated_dataset = en_translated_dataset.map(add_tl_translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "9f599ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "exceptions = [bool(not v) for v in dual_translated_dataset['tl_translation']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "21af167a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_dataset = dual_translated_dataset.select([i for i in range(len(dual_translated_dataset)) if i not in np.nonzero(exceptions)[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "36953fd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.08899927139282227,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Flattening the indices",
       "rate": null,
       "total": 62,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9054e87ba0da428fa19cc457163d3e78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/62 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sel_dataset.save_to_disk(\"data/translated_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "7dc9c552",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/translated_dataset.pkl']"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(sel_dataset, 'data/translated_dataset.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e947b1e",
   "metadata": {},
   "source": [
    "## (Deprecated) Dual Translation with Translate API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089821ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(''.join(raw_dataset['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0651e293",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect, detect_langs, LangDetectException\n",
    "import langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e38a06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_language(txts, labels):\n",
    "    en = []\n",
    "    tl = []\n",
    "    na = []\n",
    "    for txt, label in tqdm.notebook.tqdm_notebook(zip(txts, labels), total=len(txts)):\n",
    "        timer = 10\n",
    "        found = False\n",
    "        while timer:\n",
    "            try:\n",
    "                langs = detect_langs(txt)\n",
    "            except LangDetectException:\n",
    "                break\n",
    "            for l in langs:\n",
    "                if l.lang == 'en':\n",
    "                    en.append((txt, label))\n",
    "                    found = True\n",
    "                    break\n",
    "                elif l.lang == 'tl':\n",
    "                    tl.append((txt, label))\n",
    "                    found = True\n",
    "                    break\n",
    "            if found:\n",
    "                break\n",
    "            timer -= 1\n",
    "        if not found:\n",
    "            na.append((txt, label))\n",
    "    return {\n",
    "        'en': tuple(zip(*en)), 'tl': tuple(zip(*tl)), 'na': tuple(zip(*na))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71054f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sorted = sort_by_language(labeled_data['detagged'], labeled_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd485919",
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang in data_sorted:\n",
    "    data_sorted[lang] = {\n",
    "        'text': data_sorted[lang][0],\n",
    "        'labels': data_sorted[lang][1]\n",
    "    }\n",
    "    data_sorted[lang] = Dataset.from_dict(data_sorted[lang])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06a45b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "data = copy.deepcopy(data_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3359b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "tl_tokenizer = AutoTokenizer.from_pretrained(\"jcblaise/roberta-tagalog-base\")\n",
    "en_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tl_en_tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-tl-en\")\n",
    "tl_en_model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-tl-en\")\n",
    "en_tl_tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-tl\")\n",
    "en_tl_model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-tl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476d1087",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "en_tl_coll = DataCollatorForSeq2Seq(tokenizer=en_tl_tokenizer, model=en_tl_model)\n",
    "tl_en_coll = DataCollatorForSeq2Seq(tokenizer=tl_en_tokenizer, model=tl_en_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1abd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function_en_tl(examples):\n",
    "    return en_tl_tokenizer(examples['text'])\n",
    "def tokenize_function_tl_en(examples):\n",
    "    return tl_en_tokenizer(examples['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded336bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_tl_en(data, batch_size=16):\n",
    "    tokenized = data.map(tokenize_function_tl_en, remove_columns='text')\n",
    "    tokenized = tokenized.remove_columns(['labels'])\n",
    "    \n",
    "    translated = []\n",
    "    for start in tqdm_notebook(range(0, len(data), batch_size)):\n",
    "        batch = [tokenized[i] for i in range(start, min(start+batch_size, len(data)))]\n",
    "        collated = tl_en_coll(batch).to('cuda')\n",
    "        out = tl_en_model.generate(**collated)\n",
    "        translated.append(tl_en_tokenizer.batch_decode(out, skip_special_tokens=True))\n",
    "    \n",
    "    return translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f0e0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(data, tokenizer, coll, model, batch_size=16):\n",
    "    def tok_func(examples):\n",
    "        return tokenizer(examples['text'])\n",
    "    \n",
    "    tokenized = data.map(tok_func, remove_columns='text')\n",
    "    tokenized = tokenized.remove_columns(['labels'])\n",
    "    \n",
    "    translated = []\n",
    "    for start in tqdm_notebook(range(0, len(data), batch_size)):\n",
    "        batch = [tokenized[i] for i in range(start, min(start+batch_size, len(data)))]\n",
    "        collated = coll(batch).to('cuda')\n",
    "        out = model.generate(**collated)\n",
    "        translated.append(tokenizer.batch_decode(out, skip_special_tokens=True))\n",
    "    \n",
    "    return translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e387bd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tl_model.to('cpu')\n",
    "na_en_translated = func(data['na'], tl_en_tokenizer, tl_en_coll, tl_en_model)\n",
    "tl_en_model.to('cpu')\n",
    "en_tl_model.to('cuda')\n",
    "na_tl_translated = func(data['na'], en_tl_tokenizer, en_tl_coll, en_tl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc00d58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_translated = sum(en_translated, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57dacbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['na'] = data['na']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e3e3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_translated = func_tl_en(data['tl'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af41034d",
   "metadata": {},
   "source": [
    "## Finishing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff5308a",
   "metadata": {},
   "source": [
    "This is a pretty small dataset, so I'll do a 90/10 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c2541f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(\n",
    "    labeled_data['detagged'], labeled_data['label'], test_size=0.1, random_state=306)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe57dd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(X_train, './data/X_train.pkl')\n",
    "joblib.dump(X_test, './data/X_test.pkl')\n",
    "joblib.dump(y_train, './data/y_train.pkl')\n",
    "joblib.dump(y_test, './data/y_test.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70c5fca",
   "metadata": {},
   "source": [
    "# Other Preprocessing Tricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48754be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS as en_stop_words\n",
    "from spacy.lang.tl.stop_words import STOP_WORDS as tl_stop_words\n",
    "for w in (['ba','eh','kasi','lang','mo','naman','opo','po','si','talaga','yung']):\n",
    "    tl_stop_words.add(w)\n",
    "\n",
    "\n",
    "stop_words = en_stop_words.union(tl_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6698e3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text['sw_removed'] = text['tokenized'].apply(lambda l: [x for x in l if x not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfa3b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text['sw_removed']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "632a0d9eb81fa5fec1379dc19a5e552ebe5278eba4ec26ad0deb2324f235f1ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
