{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e52bc1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, ClassLabel\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "384cd5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at data/translated_dataset\\cache-0c45efaa9837687b.arrow\n",
      "Loading cached processed dataset at data/translated_dataset\\cache-b290176a263dc347.arrow\n",
      "Loading cached processed dataset at data/translated_dataset\\cache-d7ff6d7f86dd767c.arrow\n"
     ]
    }
   ],
   "source": [
    "base_dataset = load_from_disk('./data/translated_dataset')\n",
    "base_dataset = base_dataset.rename_column(\"label\", \"labels\")\n",
    "base_dataset = base_dataset.class_encode_column(\"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce996dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at data/translated_dataset\\cache-fc9934ac9315e141.arrow and data/translated_dataset\\cache-c2ec7361a7f40877.arrow\n"
     ]
    }
   ],
   "source": [
    "base_dataset = base_dataset.train_test_split(stratify_by_column='labels', seed=306, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57f37a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sample(sample, tokenizer, text_col='text'):\n",
    "    \"\"\"Appends the result of tokenizing the specified text column to the sample.\"\"\"\n",
    "    tokenized = tokenizer(sample['text'])\n",
    "    for k in tokenized:\n",
    "        sample[k] = tokenized[k]\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016274b2",
   "metadata": {},
   "source": [
    "## Model 1: roberta-tagalog-base (base text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cd9b04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_tokenizer = AutoTokenizer.from_pretrained(\"jcblaise/roberta-tagalog-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21203fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tl(sample):\n",
    "    sample = {'text': sample['text']}\n",
    "    sample = tokenize_sample(sample, tl_tokenizer)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9b99498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at data/translated_dataset\\cache-d8b03184ac0a1641.arrow\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.028012990951538086,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 7,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2637676ceb7b44bba91e33532e589824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_1 = base_dataset.map(process_tl, batched=True)\n",
    "dataset_1 = dataset_1.remove_columns(['text', 'en_translation', 'lang', 'tl_translation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18c0ee72",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_1.save_to_disk('data/dataset_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8214db47",
   "metadata": {},
   "source": [
    "## Model 2: XLM-RoBERTa-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "405cdd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlm_tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a534dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 1, 2], 'attention_mask': [1, 1, 1]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xlm_tokenizer('<pad>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11e7adc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_xlm = lambda sample: tokenize_sample(sample, xlm_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9295062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.02099752426147461,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 55,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c169042fc9a487795111ceead2253b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/55 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_2 = base_dataset.map(process_xlm, batched=True)\n",
    "dataset_2 = dataset_2.remove_columns(['text', 'en_translation', 'lang', 'tl_translation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42483db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_2.save_to_disk('data/dataset_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c37eaa8",
   "metadata": {},
   "source": [
    "## Model 3: bert-base-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7172b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8469e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_en = lambda sample: tokenize_sample(sample, en_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406ab761",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_3 = base_dataset.map(process_en, batched=True)\n",
    "dataset_3 = dataset_3.remove_columns(['text', 'en_translation', 'lang', 'tl_translation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f94d755",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_3.save_to_disk('data/dataset_3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a37bdc",
   "metadata": {},
   "source": [
    "## Model 4: Bilingual, Non-Translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3ac8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_bilingual(sample, tl_tokenizer=tl_tokenizer, en_tokenizer=en_tokenizer, tl_col='text', en_col='text'):\n",
    "    \"\"\"Appends the result of tokenizing the specified text column to the sample.\"\"\"\n",
    "    tl_tokenized = tl_tokenizer(sample[tl_col])\n",
    "    for k in tl_tokenized:\n",
    "        sample[f'tl_{k}'] = tl_tokenized[k]\n",
    "    en_tokenized = en_tokenizer(sample[en_col])\n",
    "    for k in en_tokenized:\n",
    "        sample[f'en_{k}'] = en_tokenized[k]\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7de8bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_4 = base_dataset.map(tokenize_bilingual, batched=True)\n",
    "dataset_4 = dataset_4.remove_columns(['text', 'en_translation', 'lang', 'tl_translation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5d457e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_4.save_to_disk('data/dataset_4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89b7f8f",
   "metadata": {},
   "source": [
    "## Model 5: Bilingual, Half-Translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d10d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def half_translate(sample):\n",
    "    if sample['lang'] == 'fil':\n",
    "        sample = tokenize_bilingual(sample, en_col = 'en_translation')\n",
    "    elif sample['lang'] == 'en':\n",
    "        sample = tokenize_bilingual(sample, tl_col = 'tl_translation')\n",
    "    else:\n",
    "        sample = tokenize_bilingual(sample, en_col = 'en_translation', tl_col = 'tl_translation')\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ad07cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_5 = base_dataset.map(half_translate)\n",
    "dataset_5 = dataset_5.remove_columns(['text', 'en_translation', 'lang', 'tl_translation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e46971",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_5.save_to_disk('data/dataset_5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095968bb",
   "metadata": {},
   "source": [
    "## Model 6: Bilingual, Fully Translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bea53e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_translate = lambda sample: tokenize_bilingual(sample, en_col = 'en_translation', tl_col = 'tl_translation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d770b7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_6 = base_dataset.map(full_translate)\n",
    "dataset_6 = dataset_6.remove_columns(['text', 'en_translation', 'lang', 'tl_translation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb828ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_6.save_to_disk('data/dataset_6')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_elections",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15 | packaged by conda-forge | (default, Jan 26 2023, 10:42:30) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "632a0d9eb81fa5fec1379dc19a5e552ebe5278eba4ec26ad0deb2324f235f1ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
